# -*- coding: utf-8 -*-
"""「ptt-weaviate.ipynb」的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UdZ_ZPppLu9AwOCOlUSKlTw0M-XTADmA
"""

!pip install python-dotenv
!pip install langchain
!pip install p_tqdm
!pip install
!pip install weaviate-client
!pip install OpenAI

import dataclasses
from dataclasses import dataclass
import json
from multiprocessing import Pool
from pathlib import Path
from pprint import pprint
import os

from bs4 import BeautifulSoup as bs
from dotenv import load_dotenv
from langchain.embeddings import OpenAIEmbeddings
from langchain.retrievers.weaviate_hybrid_search import WeaviateHybridSearchRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from p_tqdm import p_map
from tqdm.notebook import tqdm
import tiktoken
import weaviate
from weaviate.util import generate_uuid5

load_dotenv('../.env')
os.environ['WEAVIATE_ADMIN_PASS'] = "weaviate-ultimate-forever-pass"
os.environ['OPENAI_API_KEY'] = "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"
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'), client=None)
splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=8000, chunk_overlap=0)
encoder = tiktoken.get_encoding('cl100k_base')

ptt_posts = list(Path('/Users/cllt_finalproject/NTU').glob('**/*.xml'))
print(f'Number of posts: {len(ptt_posts)}')

# https://realpython.com/python-data-classes/
# https://realpython.com/python-type-checking/
@dataclass
class ContentItem:
    media: str  # media source of the post or comment
    content_type: str  # post or comment
    author: str  # author of the post or comment
    post_id: str  # id of the post
    year: str  # year of the post
    board: str  # board of the post
    title: str  # title of the post
    text: str  # text of the post or comment
    rating: str  # rating of the comment
    order: int  # 0 for post, 1, 2, 3, ... for comments
    chunk: int  # if text too long, split into chunks
    total_chunks: int  # total number of chunks


def get_comments(parent: ContentItem, soup: bs) -> list[ContentItem]:
    """
    Get comments from a post

    Args:
        parent: parent post
        soup: BeautifulSoup object of the post

    Returns:
        List of ContentItem objects
    """
    res = []
    comments = soup.find_all("comment")
    for comment_idx, comment in enumerate(comments, 1):
        author = comment["author"]
        rating = comment["c_type"]
        text = comment.get_text().replace("\n", "")
        chunks = splitter.split_text(text)
        if not chunks:
            chunks = [""]
        content_type = "comment"
        for chunk_idx, chunk in enumerate(chunks, 1):
            res.append(
                ContentItem(
                    media=parent.media,
                    content_type=content_type,
                    post_id=parent.post_id,
                    author=author,
                    rating=rating,
                    text=chunk,
                    year=parent.year,
                    board=parent.board,
                    title=parent.title,
                    order=comment_idx,
                    chunk=chunk_idx,
                    total_chunks=len(chunks),
                )
            )
    return res


def get_post_info(path: Path) -> list[ContentItem]:
    """
    Get post information from a post

    Args:
        path: path to the post

    Returns:
        List of ContentItem objects
    """
    with path.open() as f:
        soup = bs(f.read(), "xml")
    media = soup.find("metadata", attrs={"name": "media"}).get_text().replace("\n", "")
    content_type = "post"
    author = (
        soup.find("metadata", attrs={"name": "author"}).get_text().replace("\n", "")
    )
    post_id = (
        soup.find("metadata", attrs={"name": "post_id"}).get_text().replace("\n", "")
    )
    year = soup.find("metadata", attrs={"name": "year"}).get_text().replace("\n", "")
    board = soup.find("metadata", attrs={"name": "board"}).get_text().replace("\n", "")
    title = soup.find("metadata", attrs={"name": "title"}).get_text().replace("\n", "")
    text = soup.find("body").get_text().replace("\n", "")
    chunks = splitter.split_text(text)
    if not chunks:
        chunks = [""]
    posts = []
    for idx, chunk in enumerate(chunks, 1):
        posts.append(
            ContentItem(
                media=media,
                author=author,
                post_id=post_id,
                year=year,
                board=board,
                title=title,
                text=chunk,
                rating="",
                content_type=content_type,
                order=0,
                chunk=idx,
                total_chunks=len(chunks),
            )
        )
    if not posts:
        print(f"Empty post: {path}")
        raise ValueError(path)
    comments = get_comments(posts[0], soup)

    return posts + comments

"""# Inspect a post"""

with ptt_posts[5000].open() as f:
    soup = bs(f.read(), 'xml')
print(soup.prettify())

"""# Use multiprocessing to speed up the process"""

res = p_map(get_post_info, ptt_posts)
res = [item for sublist in res for item in sublist]
print(f'Number of posts: {len(res)}')

"""# Save"""

#wCustom JSON encoder
class EnhancedJSONEncoder(json.JSONEncoder):
    def default(self, o):
        if dataclasses.is_dataclass(o):
            return dataclasses.asdict(o)
        return super().default(o)


with open("../other_data/ptt/ptt-joined.jsonl", "x") as f:
    for r in res:
        f.write(json.dumps(r, ensure_ascii=False, cls=EnhancedJSONEncoder) + "\n")

"""# Load"""

with open('../other_data/ptt/ptt-joined.jsonl') as f:
    res = [json.loads(line) for line in f]
print(f'Number of posts: {len(res)}')

"""# Vector DB Weaviate"""

client = weaviate.Client(
    url="http://140.112.147.128:8000",
    # auth_client_secret=weaviate.AuthApiKey('cllt2023-pass'),
    auth_client_secret=weaviate.AuthApiKey(api_key=os.environ["WEAVIATE_ADMIN_PASS"]),
    timeout_config=(5, 30), # type: ignore
    additional_headers={'X-OpenAI-Api-Key': os.environ["OPENAI_API_KEY"]}
)

# https://weaviate.io/blog/pulling-back-the-curtains-on-text2vec#discussions--wrap-up
schema = {
    "class": "ContentItem",
    "description": "General content item",
    "moduleConfig": {"text2vec-openai": {"vectorizeClassName": False}},
    "vectorizer": "text2vec-openai",  # This could be any vectorizer
    "properties": [
        {
            "name": "media",
            "description": "Source of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": True,
                    "vectorizePropertyName": False,
                }
            },
        },
        {
            "name": "content_type",
            "description": "Type of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": True,
                    "vectorizePropertyName": False,
                }
            },
        },
        {
            "name": "author",
            "description": "Author of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": True,
                    "vectorizePropertyName": False,
                }
            },
        },
        {
            "name": "post_id",
            "description": "Post id of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": True,
                    "vectorizePropertyName": False,
                }
            },
        },
        {
            "name": "year",
            "description": "Year of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": True,
                    "vectorizePropertyName": False,
                }
            },
        },
        {
            "name": "board",
            "description": "Board of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": False,
                    "vectorizePropertyName": True,
                }
            },
        },
        {
            "name": "title",
            "description": "Title of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": False,
                    "vectorizePropertyName": True,
                }
            },
        },
        {
            "name": "text",
            "description": "Text of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": False,
                    "vectorizePropertyName": True,
                }
            },
        },
        {
            "name": "rating",
            "description": "Rating of the content",
            "dataType": ["text"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": False,
                    "vectorizePropertyName": True,
                }
            },
        },
        {
            "name": "order",
            "description": "0 for post, 1, 2, 3, ... for comments",
            "dataType": ["int"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": True,
                    "vectorizePropertyName": False,
                }
            },
        },
        {
            "name": "chunk",
            "description": "Chunk of the current content",
            "dataType": ["int"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": True,
                    "vectorizePropertyName": False,
                }
            },
        },
        {
            "name": "total_chunks",
            "description": "Total chunks of the content",
            "dataType": ["int"],
            "moduleConfig": {
                "text2vec-openai": {
                    "skip": True,
                    "vectorizePropertyName": False,
                }
            },
        },
    ],
}

"""# Prepare for vectorization"""

counts = []
to_vectorize = []
for r in tqdm(res):
    r = dataclasses.asdict(r)
    keep = ['board', 'rating', 'text', 'title']
    out = ""
    content_type = r['content_type']
    if content_type == 'post':
        keep.remove('rating')
    for k in keep:
        out += f"{k} {r[k].lower()} "
    counts.append(len(encoder.encode(out)))
    to_vectorize.append(out)

# {"model": "text-embedding-ada-002", "input": "embed me", "metadata": {"row_id": 1}}
with open('../other_data/ptt/ptt-joined-to-vectorize.jsonl', 'w') as f:
    for idx, r in enumerate(to_vectorize):
        f.write(json.dumps({"model": "text-embedding-ada-002", "input": r, "metadata": {"row_id": idx}}) + '\n')

to_vectorize[0]

tokens = 0
for r in tqdm(to_vectorize):
    tokens += len(encoder.encode(r))

# $0.0004 per 1000 tokens
tokens / 1000 * 0.0004

"""* Run this script to vectorize the posts: https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py

# Finding duplicates
"""

with open('../other_data/ptt/ptt-joined-to-vectorize_uploaded.json') as f:
    uploaded = json.load(f)
uploaded[0]

def get_vec(output: list[dict]) -> tuple[list[float], int]:
    embedding = output[1]['data'][0]['embedding']
    row_id = output[2]['row_id']
    return embedding, row_id

limit = 10000
selected = []

with client.batch(
    num_workers=16,
    batch_size=100,
    dynamic=True,
) as batch:
    with open('../other_data/ptt/ptt-joined-to-vectorize_results.jsonl') as f:
        for idx, line in enumerate(f):
            vector, content_idx = get_vec(json.loads(line))
            r = res[content_idx]
            selected.append(r)
            batch.add_data_object(
                data_object=r,
                class_name="ContentItem",
                uuid=generate_uuid5(r),
                vector=vector,
            )
            if idx == limit:
                break

client.query.aggregate('ContentItem').with_meta_count().do()

from collections import Counter
selected = [json.dumps(r) for r in selected]
c = Counter(selected)

list(c.values())

"""## Query
* https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/weaviate-hybrid.html
"""

!pip install python-dotenv
!pip install langchain
!pip install p_tqdm
!pip install
!pip install weaviate-client
!pip install OpenAI

import dataclasses
from dataclasses import dataclass
from pprint import pprint
import os
import weaviate
from langchain.retrievers.weaviate_hybrid_search import WeaviateHybridSearchRetriever


@dataclass
class ContentItem:
    media: str  # media source of the post or comment
    content_type: str  # post or comment
    author: str  # author of the post or comment
    post_id: str  # id of the post
    year: str  # year of the post
    board: str  # board of the post
    title: str  # title of the post
    text: str  # text of the post or comment
    rating: str  # rating of the comment
    order: int  # 0 for post, 1, 2, 3, ... for comments
    chunk: int  # if text too long, split into chunks
    total_chunks: int  # total number of chunks


os.environ["WEAVIATE_ADMIN_PASS"] = "weaviate-ultimate-forever-pass"
os.environ["OPENAI_API_KEY"] = "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"

client = weaviate.Client(
    url="http://140.112.147.128:8000",
    auth_client_secret=weaviate.AuthApiKey(api_key=os.environ["WEAVIATE_ADMIN_PASS"]),
    timeout_config=(5, 30), # type: ignore
    additional_headers={'X-OpenAI-Api-Key': os.environ["OPENAI_API_KEY"]}
)

# https://weaviate.io/blog/hybrid-search-explained
attributes = [field.name for field in dataclasses.fields(ContentItem)]
print(attributes)
retriever = WeaviateHybridSearchRetriever(
    client=client,
    k=500,
    alpha=0.9,  # weighting for each search algorithm (alpha = 0 (sparse, BM25), alpha = 1 (dense), alpha = 0.5 (equal weight for sparse and dense))
    index_name="ContentItem",
    text_key="text",
    attributes=attributes,
)

r = retriever.get_relevant_documents("")
pprint(r)

"""## Data Analysis"""

documents = r

# Extract page_content from each document
page_contents = [doc.page_content for doc in r]

content_list = page_contents
print(content_list)

!pip install transformers datasets

import re
def remove_punctuation_and_digits(strings_list):
    pattern = re.compile(r'[^\u4e00-\u9fff]+')
    result_list = []
    for string in strings_list:
        processed_string = pattern.sub('', string)
        result_list.append(processed_string)
    return result_list

input_list = content_list

filtered_content_list = remove_punctuation_and_digits(input_list)


def truncate_strings(strings_list, max_length=512):
    result_list = [string[:max_length] for string in strings_list]
    return result_list

input_list = filtered_content_list

output_content = truncate_strings(input_list, max_length=510)

!pip install transformers torch

from transformers import AutoModelForSequenceClassification , AutoTokenizer, pipeline

model_name = "liam168/c2-roberta-base-finetuned-dianping-chinese"
class_num = 2
ts_texts = output_content
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=class_num)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
content = classifier(ts_texts)
content_senti = [item['label'] for item in content if item['label'] in ['positive', 'negative']]

def count_positive_negative(strings_list):
    positive_count = strings_list.count("positive")
    negative_count = strings_list.count("negative")
    return positive_count, negative_count

input_list = content_senti

positive, negative = count_positive_negative(input_list)

print("Positive count:", positive)
print("Negative count:", negative)